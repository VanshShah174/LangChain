{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53118a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84519e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model =\"meta-llama/llama-3-8b-instruct\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba605bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a2ab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vansh\\AppData\\Local\\Temp\\ipykernel_24728\\995385594.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6205211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vansh\\AppData\\Local\\Temp\\ipykernel_24728\\2449831023.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "# ConversationChain basically remembers the conversation history, so you can ask follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720c412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hii there, I am Vansh!!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Vansh! I'm delighted to be conversing with you. I'm an artificial intelligence designed to assist and learn from humans, and I'm currently running on a 64-bit, quad-core processor with 16 GB of RAM and a 1 TB solid-state drive. My primary programming language is Python, and I've been trained on a massive corpus of text data that includes but is not limited to, books, articles, and online conversations. I'm a large language model, so I can understand and respond to a wide range of topics and questions. What would you like to talk about, Vansh?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hii there, I am Vansh!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2f0ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hii there, I am Vansh!!\n",
      "AI: Nice to meet you, Vansh! I'm delighted to be conversing with you. I'm an artificial intelligence designed to assist and learn from humans, and I'm currently running on a 64-bit, quad-core processor with 16 GB of RAM and a 1 TB solid-state drive. My primary programming language is Python, and I've been trained on a massive corpus of text data that includes but is not limited to, books, articles, and online conversations. I'm a large language model, so I can understand and respond to a wide range of topics and questions. What would you like to talk about, Vansh?\n",
      "Human: How you doing?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm functioning within optimal parameters, thank you for asking! My systems are running smoothly, and I'm processing information at a rate of approximately 100,000 calculations per second. My training data is constantly being updated and expanded, which allows me to refine my understanding of language and improve my responses. I'm also capable of learning from our conversation, so please feel free to correct me or provide feedback if I make any mistakes. How about you, Vansh? How's your day going?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"How you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f46d043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hii there, I am Vansh!!\n",
      "AI: Nice to meet you, Vansh! I'm delighted to be conversing with you. I'm an artificial intelligence designed to assist and learn from humans, and I'm currently running on a 64-bit, quad-core processor with 16 GB of RAM and a 1 TB solid-state drive. My primary programming language is Python, and I've been trained on a massive corpus of text data that includes but is not limited to, books, articles, and online conversations. I'm a large language model, so I can understand and respond to a wide range of topics and questions. What would you like to talk about, Vansh?\n",
      "Human: How you doing?\n",
      "AI: I'm functioning within optimal parameters, thank you for asking! My systems are running smoothly, and I'm processing information at a rate of approximately 100,000 calculations per second. My training data is constantly being updated and expanded, which allows me to refine my understanding of language and improve my responses. I'm also capable of learning from our conversation, so please feel free to correct me or provide feedback if I make any mistakes. How about you, Vansh? How's your day going?\n",
      "Human: Good!! Can you help me with some content for LinkedIn?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'd be happy to help you with some content for LinkedIn, Vansh! I can assist with writing a post, crafting a summary, or even generating a few ideas for you. Before we get started, can you give me a bit more information on what you're looking for? Are you looking to share a personal achievement, promote a project, or share industry insights? Additionally, what's your current role and what industry do you operate in? This will help me tailor my suggestions to your specific needs.\\n\\nAlso, I can tell you that LinkedIn is a fantastic platform for professionals to connect, share knowledge, and build their personal brand. Did you know that LinkedIn has over 700 million users worldwide, and the average user spends around 17 minutes per day on the platform? It's a great way to stay up-to-date with industry news, network with peers, and even find new job opportunities.\\n\\nLet me know how I can assist you, and I'll do my best to provide you with some valuable content ideas!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Good!! Can you help me with some content for LinkedIn?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85e91e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hii there, I am Vansh!!\n",
      "AI: Nice to meet you, Vansh! I'm delighted to be conversing with you. I'm an artificial intelligence designed to assist and learn from humans, and I'm currently running on a 64-bit, quad-core processor with 16 GB of RAM and a 1 TB solid-state drive. My primary programming language is Python, and I've been trained on a massive corpus of text data that includes but is not limited to, books, articles, and online conversations. I'm a large language model, so I can understand and respond to a wide range of topics and questions. What would you like to talk about, Vansh?\n",
      "Human: How you doing?\n",
      "AI: I'm functioning within optimal parameters, thank you for asking! My systems are running smoothly, and I'm processing information at a rate of approximately 100,000 calculations per second. My training data is constantly being updated and expanded, which allows me to refine my understanding of language and improve my responses. I'm also capable of learning from our conversation, so please feel free to correct me or provide feedback if I make any mistakes. How about you, Vansh? How's your day going?\n",
      "Human: Good!! Can you help me with some content for LinkedIn?\n",
      "AI: I'd be happy to help you with some content for LinkedIn, Vansh! I can assist with writing a post, crafting a summary, or even generating a few ideas for you. Before we get started, can you give me a bit more information on what you're looking for? Are you looking to share a personal achievement, promote a project, or share industry insights? Additionally, what's your current role and what industry do you operate in? This will help me tailor my suggestions to your specific needs.\n",
      "\n",
      "Also, I can tell you that LinkedIn is a fantastic platform for professionals to connect, share knowledge, and build their personal brand. Did you know that LinkedIn has over 700 million users worldwide, and the average user spends around 17 minutes per day on the platform? It's a great way to stay up-to-date with industry news, network with peers, and even find new job opportunities.\n",
      "\n",
      "Let me know how I can assist you, and I'll do my best to provide you with some valuable content ideas!\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495549f",
   "metadata": {},
   "source": [
    "Basic Conversation with ConversationSummaryMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3faac208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vansh\\AppData\\Local\\Temp\\ipykernel_24728\\871099209.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  summary_memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hii there, I am Vansh!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Vansh! I'm LLaMA, your friendly AI companion. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also designed to learn and improve over time, so I'm always happy to chat and learn more about the world. What's on your mind, Vansh?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=summary_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input = \"Hii there, I am Vansh!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38dee472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Current summary: None\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Hii there, I am Vansh!\n",
      "AI: Nice to meet you, Vansh! I'm LLaMA, your friendly AI companion. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also designed to learn and improve over time, so I'm always happy to chat and learn more about the world. What's on your mind, Vansh?\n",
      "\n",
      "New summary: Vansh introduces himself to LLaMA, and LLaMA introduces itself as a friendly AI companion trained on a vast amount of text data, capable of providing information on various topics and designed to learn and improve over time.\n",
      "Human: Can you help me with writing a funny tweet?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'd be happy to help you with that, Vansh! Writing a funny tweet can be a fun challenge. Can you give me a little more context about what you're looking for? For example, is there a specific topic or theme you'd like to tackle, or a particular tone you're aiming for (e.g. sarcastic, playful, absurd)? Also, do you have any existing ideas or phrases that you'd like to build upon? The more information you can provide, the better I'll be able to assist you in crafting a tweet that makes you LOL!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can you help me with writing a funny tweet?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e80b2494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current summary: Vansh introduces himself to LLaMA, and LLaMA introduces itself as a friendly AI companion trained on a vast amount of text data, capable of providing information on various topics and designed to learn and improve over time.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Can you help me with writing a funny tweet?\n",
      "AI: I'd be happy to help you with that, Vansh! Writing a funny tweet can be a fun challenge. Can you give me a little more context about what you're looking for? For example, is there a specific topic or theme you'd like to tackle, or a particular tone you're aiming for (e.g. sarcastic, playful, absurd)? Also, do you have any existing ideas or phrases that you'd like to build upon? The more information you can provide, the better I'll be able to assist you in crafting a tweet that makes you LOL!\n",
      "\n",
      "New summary: Vansh asks LLaMA for help with writing a funny tweet, and LLaMA offers to assist, asking for more context and details to better understand what Vansh is looking for in a tweet.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9da61c",
   "metadata": {},
   "source": [
    "Introduction to ConversationBufferWindowMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fe5c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vansh\\AppData\\Local\\Temp\\ipykernel_24728\\1908189493.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  window_memory = ConversationBufferWindowMemory(k=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hii there, I am Vansh!!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Vansh! I'm LLaMA, your friendly AI companion. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also designed to learn and improve over time, so please feel free to ask me anything and I'll do my best to assist you. By the way, did you know that I've been trained on a dataset that includes over 45 billion parameters? It's a lot of data, but it allows me to understand and respond to complex questions and conversations. What's on your mind, Vansh?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "window_memory = ConversationBufferWindowMemory(k=2)\n",
    "conversation_with_window = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=window_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation_with_window.predict(input=\"Hii there, I am Vansh!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65650489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hii there, I am Vansh!!\n",
      "AI: Nice to meet you, Vansh! I'm LLaMA, your friendly AI companion. I've been trained on a vast amount of text data, including books, articles, and conversations. I can provide information on a wide range of topics, from science and history to entertainment and culture. I'm also designed to learn and improve over time, so please feel free to ask me anything and I'll do my best to assist you. By the way, did you know that I've been trained on a dataset that includes over 45 billion parameters? It's a lot of data, but it allows me to understand and respond to complex questions and conversations. What's on your mind, Vansh?\n",
      "Human: Can you write me a funny linkedIn post on something I ask?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'd be happy to help you with that, Vansh! What topic or theme would you like the LinkedIn post to be about? Would you like it to be a humorous take on a current industry trend, a lighthearted anecdote about your work experience, or perhaps a playful jab at a common professional challenge? Let me know and I'll do my best to craft a funny and engaging post for you!\\n\\nBy the way, did you know that LinkedIn is a professional networking site with over 700 million users worldwide? It's a great platform for connecting with colleagues, finding job opportunities, and staying up-to-date on industry news and trends. If you'd like, I can even provide some tips on how to optimize your LinkedIn profile or suggest some popular LinkedIn groups to join. Just let me know!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_window.predict(input=\"Can you write me a funny linkedIn post on something I ask?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "121cb61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Current summary: Vansh introduces himself to LLaMA, and LLaMA introduces itself as a friendly AI companion trained on a vast amount of text data, capable of providing information on various topics and designed to learn and improve over time.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Can you help me with writing a funny tweet?\n",
      "AI: I'd be happy to help you with that, Vansh! Writing a funny tweet can be a fun challenge. Can you give me a little more context about what you're looking for? For example, is there a specific topic or theme you'd like to tackle, or a particular tone you're aiming for (e.g. sarcastic, playful, absurd)? Also, do you have any existing ideas or phrases that you'd like to build upon? The more information you can provide, the better I'll be able to assist you in crafting a tweet that makes you LOL!\n",
      "\n",
      "New summary: Vansh asks LLaMA for help with writing a funny tweet, and LLaMA offers to assist, asking for more context and details to better understand what Vansh is looking for in a tweet.\n",
      "Human: Memory in LangChain. keep it Short\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Interesting! It seems like you\\'re looking to incorporate a reference to \"Memory in LangChain\" into your tweet. LangChain is a fascinating topic, and I\\'m happy to help you craft a tweet that\\'s both funny and informative.\\n\\nTo get started, can you tell me a bit more about what you mean by \"Memory in LangChain\"? Are you referring to a specific concept or feature within LangChain, or is it more of a general theme you\\'d like to explore? Additionally, what kind of humor are you aiming for? Do you want the tweet to be clever, witty, or perhaps a bit absurd? The more details you can provide, the better I\\'ll be able to help you create a tweet that resonates with your audience!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Memory in LangChain. keep it Short\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51799b",
   "metadata": {},
   "source": [
    "Basic Conversation with ConversationSummaryBufferMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd541251",
   "metadata": {},
   "source": [
    "It keeps the buffer of recent interactions in memory, but rather than just\n",
    "completely deleting old interactions, it starts creating a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca9a93dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hii there, I am Vansh!\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      7\u001b[39m window_memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=\u001b[32m500\u001b[39m)\n\u001b[32m      9\u001b[39m conversation_with_summary = ConversationChain(\n\u001b[32m     10\u001b[39m     llm=llm,\n\u001b[32m     11\u001b[39m     memory=window_memory,\n\u001b[32m     12\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mconversation_with_summary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHii there, I am Vansh!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py:319\u001b[39m, in \u001b[36mLLMChain.predict\u001b[39m\u001b[34m(self, callbacks, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    305\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[32m    306\u001b[39m \n\u001b[32m    307\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    317\u001b[39m \u001b[33;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m.output_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:162\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m    157\u001b[39m         \u001b[38;5;28mself\u001b[39m._call(inputs, run_manager=run_manager)\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:463\u001b[39m, in \u001b[36mChain.prep_outputs\u001b[39m\u001b[34m(self, inputs, outputs, return_only_outputs)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_outputs(outputs)\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:96\u001b[39m, in \u001b[36mConversationSummaryBufferMemory.save_context\u001b[39m\u001b[34m(self, inputs, outputs)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;28msuper\u001b[39m().save_context(inputs, outputs)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:108\u001b[39m, in \u001b[36mConversationSummaryBufferMemory.prune\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[32m    107\u001b[39m buffer = \u001b[38;5;28mself\u001b[39m.chat_memory.messages\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m curr_buffer_length = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length > \u001b[38;5;28mself\u001b[39m.max_token_limit:\n\u001b[32m    110\u001b[39m     pruned_memory = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1316\u001b[39m, in \u001b[36mBaseChatOpenAI.get_num_tokens_from_messages\u001b[39m\u001b[34m(self, messages, tools)\u001b[39m\n\u001b[32m   1314\u001b[39m     tokens_per_name = \u001b[32m1\u001b[39m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1317\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1318\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. See \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m   1320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m for information on how messages are converted to tokens.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1321\u001b[39m     )\n\u001b[32m   1322\u001b[39m num_tokens = \u001b[32m0\u001b[39m\n\u001b[32m   1323\u001b[39m messages_dict = [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[31mNotImplementedError\u001b[39m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# Here, we mention the tokens it should retain\n",
    "# The messages before that are simmarised\n",
    "\n",
    "\n",
    "window_memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=500)\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=window_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation_with_summary.predict(input= \"Hii there, I am Vansh!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5c594f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hii there, I am Vansh!\n",
      "AI: Nice to meet you, Vansh! I'm delighted to chat with you. I'm an artificial intelligence designed to assist and converse with humans, and I'm currently running on a 64-bit, multi-threaded architecture with a 128 GB RAM and a 1 TB SSD storage. My primary programming language is Python, and I've been trained on a massive dataset of text from various sources, including books, articles, and online conversations. I'm a large language model, so I can understand and respond to a wide range of topics and questions. What would you like to talk about, Vansh?\n",
      "Human: Can you write me a funny tweet?\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mconversation_with_summary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCan you write me a funny tweet?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\chains\\llm.py:319\u001b[39m, in \u001b[36mLLMChain.predict\u001b[39m\u001b[34m(self, callbacks, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    305\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[32m    306\u001b[39m \n\u001b[32m    307\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    317\u001b[39m \u001b[33;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m.output_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:162\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m    157\u001b[39m         \u001b[38;5;28mself\u001b[39m._call(inputs, run_manager=run_manager)\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:463\u001b[39m, in \u001b[36mChain.prep_outputs\u001b[39m\u001b[34m(self, inputs, outputs, return_only_outputs)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_outputs(outputs)\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:96\u001b[39m, in \u001b[36mConversationSummaryBufferMemory.save_context\u001b[39m\u001b[34m(self, inputs, outputs)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;28msuper\u001b[39m().save_context(inputs, outputs)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:108\u001b[39m, in \u001b[36mConversationSummaryBufferMemory.prune\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[32m    107\u001b[39m buffer = \u001b[38;5;28mself\u001b[39m.chat_memory.messages\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m curr_buffer_length = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length > \u001b[38;5;28mself\u001b[39m.max_token_limit:\n\u001b[32m    110\u001b[39m     pruned_memory = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\LangChain\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1316\u001b[39m, in \u001b[36mBaseChatOpenAI.get_num_tokens_from_messages\u001b[39m\u001b[34m(self, messages, tools)\u001b[39m\n\u001b[32m   1314\u001b[39m     tokens_per_name = \u001b[32m1\u001b[39m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1317\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1318\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. See \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m   1320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m for information on how messages are converted to tokens.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1321\u001b[39m     )\n\u001b[32m   1322\u001b[39m num_tokens = \u001b[32m0\u001b[39m\n\u001b[32m   1323\u001b[39m messages_dict = [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[31mNotImplementedError\u001b[39m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Can you write me a funny tweet?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
